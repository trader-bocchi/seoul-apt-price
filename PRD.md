# 네이버 부동산 매물 수집 시스템 PRD (새로운 크롤링 방법)

## 1. 프로젝트 개요

### 1.1 목적
네이버 부동산 API를 활용하여 사용자가 입력한 **행정구역(동 단위)의 모든 부동산 매물 정보**를 효율적으로 수집하는 시스템을 개발합니다.

**핵심 목표**: 
- 입력된 행정구역의 모든 매물을 누락 없이 수집
- 기존 그리드 방식보다 훨씬 적은 API 호출로 효율적인 수집
- 네이버 부동산에서 실제로 표시하는 매물과 동일한 데이터 수집

### 1.2 목표 사용자
- 부동산 투자자
- 부동산 중개업자
- 부동산 시장 분석가
- 특정 지역의 매물 정보가 필요한 사용자

### 1.3 핵심 가치
- **간편한 행정구역 입력**으로 해당 지역의 모든 매물 정보 수집
- **효율적인 수집**: 페이지네이션 기반으로 최소한의 API 호출
- **정확한 데이터**: 네이버 부동산에서 실제로 표시하는 매물과 동일
- **CSV 기반 데이터 저장** 및 분석 가능

### 1.4 새로운 크롤링 방법의 특징

기존 그리드 방식과 달리, 사용자가 네이버 부동산에서 **"해당 지역만 보기"를 선택하고 "매물목록"을 클릭했을 때** 사용되는 API를 직접 활용합니다.

**장점:**
- API 호출 횟수가 크게 감소 (그리드 방식: 수십~수백 회 → 새로운 방식: 페이지 수만큼)
- 복잡한 클러스터 처리나 영역 분할 불필요
- 명확한 페이지네이션 구조
- 실제 네이버 부동산 페이지와 동일한 데이터

## 2. 기능 요구사항

### 2.1 행정구역 입력 및 지역 정보 조회

#### 2.1.1 .env 파일 기반 행정구역 관리
- 행정구역 정보는 코드에 하드코딩하지 않고 `.env` 파일에 저장
- `.env` 파일에는 다음 값이 존재해야 함:
  ```
  REGION_NAME=성남시 수정구 신흥동
  ```
- 행정구역명 형식: "시/도 시/군/구 동" (예: "성남시 수정구 신흥동")

#### 2.1.2 지역 정보 조회
- 행정구역명을 입력받아 다음 정보를 조회:
  - `cortarNo`: 지역 코드 (예: 4113110100)
  - 중심 좌표 (위도, 경도)
  - 지역 경계 좌표 (btm, lft, top, rgt)

**조회 방법:**
1. Geopy를 사용하여 행정구역명을 좌표로 변환
2. 해당 좌표로 `cluster/clusterList` API를 호출하여 `cortarNo` 조회

### 2.2 매물 수집 옵션

#### 2.2.1 부동산 유형 선택
- 아파트 (APT)
- 오피스텔 (OPST)
- 빌라 (VL)
- 기타

#### 2.2.2 거래 유형 선택
- 매매 (A1)
- 전세 (B1)
- 월세 (B2)

#### 2.2.3 기본 설정
- 기본 부동산 유형: 아파트 (APT)
- 기본 거래 유형: 매매 (A1)
- 기본 줌 레벨: 14

### 2.3 매물 수집 실행

#### 2.3.1 수집 프로세스
1. **지역 정보 조회** (0-10%)
   - 행정구역명으로 `cortarNo`와 좌표 조회
   - 경계 좌표 계산

2. **첫 페이지 조회** (10-20%)
   - `cluster/ajax/articleList` API로 첫 페이지 조회
   - 총 매물 개수 확인 (가능한 경우)

3. **페이지네이션 수집** (20-95%)
   - `more` 필드나 페이지별 매물 개수를 확인
   - 모든 페이지를 순차적으로 수집
   - 각 페이지당 최대 20개 매물

4. **데이터 정제** (95-100%)
   - 중복 매물 제거 (itemId 기준)
   - 데이터 정제 및 저장

#### 2.3.2 진행 상황 표시
- 진행률 프로그레스 바 (0-100%)
- 현재 수집 중인 페이지 표시
- 수집된 매물 개수 실시간 업데이트
- 예상 완료 시간 (선택사항)

#### 2.3.3 수집 로직 상세

**API 호출 구조:**
```
GET https://m.land.naver.com/cluster/ajax/articleList
  ?rletTpCd=APT
  &tradTpCd=A1
  &z=14
  &lat={center_lat}
  &lon={center_lon}
  &btm={btm}
  &lft={lft}
  &top={top}
  &rgt={rgt}
  &cortarNo={cortarNo}
  &page={page_number}
```

**페이지네이션 처리:**
- 첫 페이지: `page` 파라미터 없이 호출 (또는 `page=1`)
- 다음 페이지: `page=2`, `page=3`, ... 순차적으로 호출
- 종료 조건:
  - `more` 필드가 `false`이고 현재 페이지 매물이 20개 미만
  - 빈 페이지 응답
  - 최대 페이지 수 도달 (안전장치)

### 2.4 Raw 데이터 저장

#### 2.4.1 저장 규칙
- 모든 수집 데이터는 CSV 형태로 저장
- 데이터베이스 사용 금지
- 수집 일시를 파일명에 포함

#### 2.4.2 저장 경로
```
data/raw/{region_name}/offers_YYYYMMDD.csv
```

#### 2.4.3 CSV 구조
- `offers_YYYYMMDD.csv`: 호가 정보 (매물 상세)
  - 컬럼: `item_id`, `region_name`, `complex_name`, `property_type`, `trade_type`, `trade_type_code`, `price`, `price_display`, `latitude`, `longitude`, `min_mvi_fee`, `max_mvi_fee`, `tour_exist`, `collected_at`

### 2.5 결과 요약 및 출력

#### 2.5.1 수집 결과 요약
- 총 수집된 매물 개수
- 가격 통계 (최저가, 최고가, 평균가, 표본 수)
- 단지별 매물 수 (상위 10개)
- 수집 완료 시간

#### 2.5.2 콘솔 출력
- 진행 상황 실시간 출력
- 수집 완료 후 결과 요약 출력
- 저장된 파일 경로 출력

## 3. 기술 스택

### 3.1 필수 기술 스택
- Python 3.8+
- requests: HTTP 요청
- pandas: 데이터 처리 및 CSV 저장
- python-dotenv: .env 파일 로딩
- geopy: 주소 → 좌표 변환

### 3.2 선택 기술 스택
- datetime: 날짜/시간 처리
- pathlib: 파일 경로 처리

## 4. 프로젝트 폴더 구조

```
project_root/
├─ src/
│ ├─ collectors/
│ │  ├─ __init__.py
│ │  ├─ api_client.py          # API 클라이언트 (기존 + 새로운 메서드)
│ │  ├─ data_collector.py      # 기존 수집 로직 (참고용)
│ │  └─ region_collector.py    # 새로운 행정구역 기반 수집 로직
│ ├─ config/
│ │  ├─ __init__.py
│ │  └─ env_loader.py          # .env 파일 로딩 (행정구역명 추가)
│ └─ storage/
│    ├─ __init__.py
│    └─ csv_store.py           # CSV 저장 공통 유틸
├─ data/
│ └─ raw/                      # Raw 수집 데이터
│    └─ {region_name}/
│       └─ offers_YYYYMMDD.csv
├─ ApiRef.md                   # 새로운 API 참조 문서
├─ PRD.md                      # 제품 요구사항 문서 (이 파일)
├─ collect_by_region.py        # 새로운 수집 메인 파일
├─ .env.example                # .env 파일 예시
├─ .env                        # 실제 환경 변수 (gitignore)
├─ requirements.txt
└─ README.md
```

## 5. 데이터 흐름

### 5.1 수집 프로세스
1. `.env` 파일에서 행정구역명 로드 (`REGION_NAME`)
2. 행정구역명으로 지역 정보 조회 (`cortarNo`, 좌표)
3. 경계 좌표 계산
4. 첫 페이지 조회 (`cluster/ajax/articleList` API)
5. 페이지네이션으로 모든 페이지 수집
6. 중복 제거 및 데이터 정제
7. Raw 데이터 CSV 저장 (`data/raw/{region_name}/offers_YYYYMMDD.csv`)
8. 결과 요약 출력

### 5.2 API 호출 흐름
```
1. Geopy: 행정구역명 → 좌표 변환
2. cluster/clusterList: 좌표 → cortarNo 조회
3. cluster/ajax/articleList (page=1): 첫 페이지 조회
4. cluster/ajax/articleList (page=2): 두 번째 페이지 조회
5. cluster/ajax/articleList (page=3): 세 번째 페이지 조회
...
N. cluster/ajax/ajax/articleList (page=N): 마지막 페이지 조회
```

## 6. 구현 세부사항

### 6.1 API 클라이언트 확장

**새로운 메서드 추가:**
- `get_article_list_by_region()`: 지역별 매물 목록 조회 (페이지네이션 지원)
- `search_region_info()`: 행정구역명으로 지역 정보 조회

### 6.2 수집기 구현

**새로운 클래스:**
- `RegionCollector`: 행정구역 기반 매물 수집기
  - `get_region_info()`: 지역 정보 조회
  - `calculate_region_bounds()`: 경계 좌표 계산
  - `extract_properties_from_article_list()`: API 응답에서 매물 추출
  - `collect_properties_by_region()`: 전체 수집 프로세스

### 6.3 환경 변수 관리

**새로운 환경 변수:**
- `REGION_NAME`: 행정구역명 (예: "성남시 수정구 신흥동")

**기존 환경 변수 (선택사항):**
- `CURRENT_HOME_COMPLEX_NAME`: 현재 거주 단지명 (향후 확장용)
- `TARGET_HOME_COMPLEX_NAME`: 관심 단지명 (향후 확장용)

### 6.4 메인 실행 파일

**새로운 파일:**
- `collect_by_region.py`: 행정구역 기반 수집 메인 파일
  - 환경 변수 검증
  - 수집기 초기화
  - 수집 실행
  - 데이터 저장
  - 결과 요약 출력

## 7. 사용 방법

### 7.1 환경 설정

1. `.env` 파일 생성:
```bash
REGION_NAME=성남시 수정구 신흥동
```

2. 필요한 패키지 설치:
```bash
pip install -r requirements.txt
```

### 7.2 실행

```bash
python collect_by_region.py
```

### 7.3 결과 확인

수집된 데이터는 다음 경로에 저장됩니다:
```
data/raw/{region_name}/offers_YYYYMMDD.csv
```

## 8. 구현 제약사항

### 8.1 데이터베이스 미사용
- 모든 데이터는 CSV 파일로 저장
- 데이터베이스 관련 라이브러리 사용 금지

### 8.2 하드코딩 금지
- 행정구역명은 .env 파일에서만 로드
- 코드에 행정구역명 직접 입력 금지

### 8.3 API 호출 제한
- 적절한 딜레이를 두고 요청 (기본 1초)
- 재시도 로직 구현 (최대 3회)
- 타임아웃 설정 (10초)

## 9. 향후 확장 가능성

### 9.1 관심 단지 필터링
- 수집한 매물 중 특정 단지만 필터링하여 저장
- `.env`에 관심 단지명 설정

### 9.2 텔레그램 알림
- 수집 완료 시 텔레그램으로 알림 전송
- 가격 변동 알림 (향후 확장)

### 9.3 가격 분석
- 수집한 데이터를 기반으로 가격 분석
- 통계 및 시각화

### 9.4 배치 실행
- 정기적으로 자동 수집 실행
- 스케줄러 연동

## 10. 기존 방법과의 비교

### 10.1 기존 방법 (그리드 방식)
- **장점**: 넓은 영역을 체계적으로 수집 가능
- **단점**: 많은 API 호출 필요, 복잡한 로직, 클러스터 처리 필요

### 10.2 새로운 방법 (지역 기반 페이지네이션)
- **장점**: 적은 API 호출, 간단한 로직, 실제 페이지와 동일한 데이터
- **단점**: 행정구역 단위로만 수집 가능 (더 넓은 영역 수집 시 여러 번 실행 필요)

## 11. 주의사항

1. **API 접근 제한**: 과도한 요청은 IP 차단을 유발할 수 있으므로 적절한 딜레이를 두고 요청하세요.
2. **법적 고지**: 이 시스템은 네이버의 공식 API가 아닐 수 있으며, 웹사이트 구조 변경에 따라 동작하지 않을 수 있습니다. 스크래핑 시 관련 법규를 준수하세요.
3. **데이터 정확성**: 수집한 데이터는 참고용이며, 실제 거래 시에는 공식 채널을 통해 확인하세요.

